{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfc4ee8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b7f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gllekk/Documents/AI/American_Perception/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-05 13:27:10 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-05 13:27:10 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-05 13:27:11 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a295fe4",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.csv')\n",
    "print(f\"Total comments: {len(df)}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=['comment']).reset_index(drop=True)\n",
    "df = df[df['comment'].str.strip().str.len() > 10].reset_index(drop=True)\n",
    "print(f\"After cleaning: {len(df)}\")\n",
    "\n",
    "sample_size = min(500, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "print(f\"\\nUsing {sample_size} samples for manual validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae656d4",
   "metadata": {},
   "source": [
    "### Setup Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    gpu_memory_utilization=0.75,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=1,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe8b39",
   "metadata": {},
   "source": [
    "### Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(comment):\n",
    "    prompt = f\"\"\"Classify the stance of this comment about Ukraine-Russia conflict.\n",
    "\n",
    "Return ONLY one word: prorussian, neutral, or proukrainian\n",
    "\n",
    "Guidelines:\n",
    "- prorussian: supports Russia, criticizes Ukraine/Zelenskyy, justifies invasion\n",
    "- proukrainian: supports Ukraine, criticizes Russia/Putin, condemns invasion\n",
    "- neutral: balanced view, no clear stance, or discusses both sides equally\n",
    "\n",
    "Consider sarcasm and irony when present.\n",
    "\n",
    "Examples:\n",
    "\"Slava Ukraini!\" → proukrainian\n",
    "\"Putin is a great leader\" → prorussian\n",
    "\"'Great strategist' Putin lost again lol\" → proukrainian\n",
    "\"War hurts everyone\" → neutral\n",
    "\"Zelenskyy sells Ukraine to NATO\" → prorussian\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7db15",
   "metadata": {},
   "source": [
    "### Run inference on validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [create_prompt(c) for c in df_sample['comment'].tolist()]\n",
    "sampling_params = SamplingParams(temperature=0.2, max_tokens=20, stop=[\"\\n\", \".\", \",\"])\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "labels = []\n",
    "raw_outputs = []\n",
    "\n",
    "for output in outputs:\n",
    "    text = output.outputs[0].text.strip().lower()\n",
    "    raw_outputs.append(text)\n",
    "\n",
    "    if 'proukrainian' in text or 'pro-ukrainian' in text or 'ukrainian' in text:\n",
    "        labels.append('proukrainian')\n",
    "    elif 'prorussian' in text or 'pro-russian' in text or 'russian' in text:\n",
    "        labels.append('prorussian')\n",
    "    else:\n",
    "        labels.append('neutral')\n",
    "\n",
    "df_sample['stance_label'] = labels\n",
    "df_sample['raw_output'] = raw_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7312a",
   "metadata": {},
   "source": [
    "### Distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_sample['stance_label'].value_counts()\n",
    "print(counts)\n",
    "print(f\"\\nPercentages:\")\n",
    "for label, count in counts.items():\n",
    "    print(f\"  {label}: {count/len(df_sample)*100:.1f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "counts.plot(kind='bar', ax=axes[0], color=['#d62728', '#7f7f7f', '#2ca02c'])\n",
    "axes[0].set_title('Stance Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Stance')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#d62728', '#7f7f7f', '#2ca02c'])\n",
    "axes[1].set_title('Stance Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bce3a",
   "metadata": {},
   "source": [
    "### Split into train/test for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a571f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42, stratify=df_sample['stance_label'])\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "test_comments = test_df['comment'].tolist()\n",
    "test_prompts = [create_prompt(c) for c in test_comments]\n",
    "\n",
    "print(f\"Running inference on test set ({len(test_df)} samples)...\")\n",
    "test_outputs = llm.generate(test_prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "test_predictions = []\n",
    "for output in test_outputs:\n",
    "    text = output.outputs[0].text.strip().lower()\n",
    "    if 'proukrainian' in text or 'pro-ukrainian' in text or 'ukrainian' in text:\n",
    "        test_predictions.append('proukrainian')\n",
    "    elif 'prorussian' in text or 'pro-russian' in text or 'russian' in text:\n",
    "        test_predictions.append('prorussian')\n",
    "    else:\n",
    "        test_predictions.append('neutral')\n",
    "\n",
    "test_df['predicted_stance'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df['stance_label'].values\n",
    "y_pred = test_df['predicted_stance'].values\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 (macro): {f1_macro:.3f}\")\n",
    "print(f\"F1 (weighted): {f1_weighted:.3f}\")\n",
    "\n",
    "print(f\"\\n{classification_report(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bc8cd",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=['prorussian', 'neutral', 'proukrainian'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['prorussian', 'neutral', 'proukrainian'],\n",
    "            yticklabels=['prorussian', 'neutral', 'proukrainian'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, label in enumerate(['prorussian', 'neutral', 'proukrainian']):\n",
    "    print(f\"  {label}: {per_class_accuracy[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597a384",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = test_df[test_df['stance_label'] != test_df['predicted_stance']].copy()\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ERROR ANALYSIS - {len(errors)} errors out of {len(test_df)} ({len(errors)/len(test_df)*100:.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(errors) > 0:\n",
    "    print(f\"\\nError breakdown:\")\n",
    "    error_pairs = errors.groupby(['stance_label', 'predicted_stance']).size().sort_values(ascending=False)\n",
    "    for (true_label, pred_label), count in error_pairs.items():\n",
    "        print(f\"  {true_label} → {pred_label}: {count}\")\n",
    "\n",
    "    print(f\"\\nExample errors (showing up to 10):\")\n",
    "    for idx, row in errors.head(10).iterrows():\n",
    "        comment = row['comment'][:80] + '...' if len(row['comment']) > 80 else row['comment']\n",
    "        print(f\"\\nComment: {comment}\")\n",
    "        print(f\"True: {row['stance_label']} | Predicted: {row['predicted_stance']}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860455a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = [create_prompt(c) for c in df['comment'].tolist()]\n",
    "all_outputs = llm.generate(all_prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "all_labels = []\n",
    "for output in all_outputs:\n",
    "    text = output.outputs[0].text.strip().lower()\n",
    "    if 'proukrainian' in text or 'pro-ukrainian' in text or 'ukrainian' in text:\n",
    "        all_labels.append('proukrainian')\n",
    "    elif 'prorussian' in text or 'pro-russian' in text or 'russian' in text:\n",
    "        all_labels.append('prorussian')\n",
    "    else:\n",
    "        all_labels.append('neutral')\n",
    "\n",
    "df['stance_label'] = all_labels\n",
    "\n",
    "df.to_csv('data/final_dataset_with_stance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5aa3c3",
   "metadata": {},
   "source": [
    "### Final statistics and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts = df['stance_label'].value_counts()\n",
    "print(f\"\\n{final_counts}\")\n",
    "print(f\"\\nPercentages:\")\n",
    "for label, count in final_counts.items():\n",
    "    print(f\"  {label}: {count/len(df)*100:.1f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "final_counts.plot(kind='bar', ax=axes[0, 0], color=['#d62728', '#7f7f7f', '#2ca02c'])\n",
    "axes[0, 0].set_title('Final Dataset - Stance Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Stance')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "final_counts.plot(kind='pie', ax=axes[0, 1], autopct='%1.1f%%', colors=['#d62728', '#7f7f7f', '#2ca02c'])\n",
    "axes[0, 1].set_title('Final Dataset - Stance Proportion', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('')\n",
    "\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Validation Sample': df_sample['stance_label'].value_counts(),\n",
    "    'Full Dataset': final_counts\n",
    "}).fillna(0)\n",
    "\n",
    "comparison_data.plot(kind='bar', ax=axes[1, 0], color=['#1f77b4', '#ff7f0e'])\n",
    "axes[1, 0].set_title('Validation vs Full Dataset', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Stance')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=['prorussian', 'neutral', 'proukrainian'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1, 1],\n",
    "            xticklabels=['prorussian', 'neutral', 'proukrainian'],\n",
    "            yticklabels=['prorussian', 'neutral', 'proukrainian'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[1, 1].set_title('Confusion Matrix (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('True')\n",
    "axes[1, 1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
